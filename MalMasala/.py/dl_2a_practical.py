# -*- coding: utf-8 -*-
"""DL_2A_Practical.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vWcA8sSMJR1sv10mLZJ5h4rP7mGwBP-q
"""

import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense,Dropout
from keras.optimizers import RMSprop
from keras.datasets import mnist 
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.model_selection import train_test_split
from keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

df=pd.read_csv('LetterRecognition.csv')

df

df.dtypes

x=df.drop('lettr',axis=1).values
y=df['lettr'].values

print(x.shape)
print(y.shape)

x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42,test_size=0.20)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

x_train[0]

y_train[0]

x_train=x_train/255
x_test=x_test/255

x_train[0]

encoder=LabelEncoder()
y_train=encoder.fit_transform(y_train)
y_test=encoder.fit_transform(y_test)

model = Sequential()
model.add(Dense(512,activation='relu',input_shape=(16,)))
model.add(Dense(256,activation='relu'))
model.add(Dense(26,activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
batch_size=128
epochs=50

history=model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test,y_test))

x_test[100]

lab=encoder.inverse_transform([y_test[100]])
print("Actual Label : ",lab[0])

result=model.predict(x_test)

print(result[10])

final_value=np.argmax(result[10])

print("Actual label ",y_test[10])
print("Predicted label",final_value)

actual=encoder.inverse_transform([y_test[10]])
predicted=encoder.inverse_transform([final_value])

print("Actual label ",actual)
print("Predicted label",predicted)